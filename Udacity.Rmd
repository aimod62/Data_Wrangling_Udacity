---
title: "Data Wrangling Project - UDACITY"
author: "AIMOD"
date: "September 5, 2017"
output:
  html_document:
    self_contained: yes
    toc: true
    number_sections: true
---

#Parsing Naples

Naples, Italy is the area of my choice. Situated in the region of Campania, this municipality prevails as one of the largest Mediterranean cities with around 4.4 million people living in the metropolitan area. The port of Naples is ranked as the world's second highest level of passenger flow just after Hong-Kong. In the last decades, it has significantly expanded the transport infrastructure by developing the subway network and a high-speed rail linking the cities of Salerno and Roma. 
With such an influx of people, and the ongoing enlargement, it could became extremely handy to have access to a database which can provide directions on where to find a pizza, a planetarium, or the nightclub of your taste (who dares to say that Data Science is boring?). My efforts head in that direction, and in order to do so I have overpassed the surface elements: tag, node, way, and relation from the OSM file, and struggled to dig deeper in search of the attributes of interest.

## The OSM File

The map has been downloaded from The Open Street Map as an OSM XML file of a decompressed size of 62.7 MB fulfilling the requirements of the assignment prompt.^[See attached text file for map's further information.]

###Parsing

The function **etree.iterparse()** from the **lxml** model has been used to access the events of interests. Attention has been focused on *child attributes*,  and not in *parents elements* which have been eventually deleted and records duly cleaned to improve computer performance. Given the excessive amount of rows with missing data in these levels, the output has been filtered.

### Capturing Events of Interest

The following function aims to capture the element of our choice. In the present assignment I have focused on *cuisine*, but the script is equally useful for any other given attribute: *shop*, *amenity*, *clothing*, etc. I favor lists comprehension to iterate in search of a given string; however, being a Phyton novice, sometimes the syntax proved to be difficult to grasp; I came back to the traditional for loop, then.  The output has been narrowed based on the most common keys obtained via a Counter from the collections module.^[See attached standalone.py script for a complete depiction of functions used.]

```{python, eval=FALSE, echo=TRUE}

def select_entry(data, k):
    return[dict((k,v) for (k,v) in d.items()) for d in data] 
    
from collections import Counter
def most_common_keys(data, int):
    keys = [k for d in data for k in d.keys()]
    print(Counter(keys).most_common(int))
        
```

### Data Cleanup

####Foreign Characters 

Being a file about Naples, It does not come as a surprise the existence of Italian street names aka multiple messages indicating that *ascii* could not be decode or vice versa. Although, I managed to save it to a csv file by encoding in *latin-1*; it became a few days task to try to solve the message received when trying to load data into the sql database: *You must not use 8-bit bytestrings ...* Neither the use of *text_factory == str*, nor the *codecs module* helped me out;  *Python2* made the matters more difficult as Unicode encoding does not occur automatically and there were no valid utf-8 representations.  I was forced to migrate to *Python3* which handles this kind of problems without further ado. I got my csv file, converted to a Pandas data frame to facilitate some basic cleanup functions as replacing missing values by NaN or renaming columns, and subsequently writing the data to a csv file with a *latin-1* encoding.

#### Standardizing Phone Numbers.

One feature that demanded extra attenton was the phone numbers as the original data presented different patterns: white spaces, Italy's international prefix not included, etc. To the best of my knowledge a valid phone number must contain: the country code for Italy (39), the Naples area code (089) and a seven digits phone number. 
A first approach through a *re.compile* pattern searching showed that out of the 25 appointed phone numbers in the 121 selected dictionaries, just two match the pattern.

The following functions were applied incrementaly to the column phone to standardize the phone numbers and remove the inappropriate ones. Although, the three functions could be coerced into one function; I segregated them for the sake of a better control check

```{python, eval=FALSE, echo=TRUE}
def format_phone(data):
    phone = str(data)
    clean_phone  = re.sub('\+39\s{1}081\s{1}\d{7}', phone.replace(' ', ''), phone)
    return clean_phone
     
def format_phone_1(data):
    phone = str(data)
    clean_phone = re.sub('^081\d{7}', '+39{}'.format(phone), phone)
    return clean_phone

def format_phone_final(data):
    phone = str(data)
    m =  re.search(pattern_phone, phone)
    if m is not None:
        return m.group(0)
```

##The Database

I decided to use **sqlalchemy** to query the data in an efficient, and as much as possible, a *Pythonic* way. A table of 121 dictionaires containing the string *cuisine* was created. The columns *amenity* and *cuisine* were explicitly declared as *Index* to speed the subsequent queries. The list of keys is as follows: 

['amenity', 'name', 'cuisine', 'opening_hours', 'street', 'housenumber', 'city', 'postcode', 'phone', 'website', 'internet_access', 'smoking', 'wheelchair'] 

As the table's objective is to provide a database of possible food providers in the city of Naples; one our first inquiries aimes to find the different non null types of amenities, as shown by the following display:
 
![](amenity.PNG)

Taking in account the above mentioned results, a potential user might be interested to find out what type of cuisine is being offered by the 102 restaurants described in the table. The proper query gives us 30 distinct values. For illustration purposes, a few of them are included:

```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- " 
| Cuisine  | Quantity | 
|----------|:--------:|
|  italian | 30       |
|  pizza   | 38       |  
|  kebab   | 2        |
"
cat(tabl) 
```

It might be also the case that a tourist in Naples is craving for seafood. The following query suffices to supply the expected answers: 

```{sql, eval = FALSE, echo = TRUE}
from sqlalchemy import and_
stmt = select([naples_cuisine.columns.name, naples_cuisine.columns.street,
              naples_cuisine.columns.housenumber]).where(
        and_(
                naples_cuisine.columns.amenity == 'restaurant',
                naples_cuisine.columns.cuisine.contains('seafood')
                 )
        )

```

I limit the query to the 10 first results:

[('Baccalaria', 'Piazzetta di Porto', '4'), ('Da Corrado', 'Via Michele Tenore', '1'), ('Da Patrizia', 'Borgo Marinari', '24'), ('La Lazzara', 'Piazza Francese', '9/10')]

All along, my objective has been to make tangible the skills learned on the Data Science Nanodegree's wrangling section. The following function brings me closer to my goal. A function designed to query the pertinent database to find the name, address and if provided, the phone number of a given location described in the original OSM file. As previously stated, I centered my inquire on the *cuisine* rubric, but any attribute of choice will do the deed.

```{sql, eval = FALSE, echo = TRUE}
def what_do_you_want_to_eat(type_of_food, integer):
    stmt = select([naples_cuisine.columns.name,
                   naples_cuisine.columns.amenity,
                   naples_cuisine.columns.street,
                   naples_cuisine.columns.phone])
    stmt = stmt.where(naples_cuisine.columns.cuisine == type_of_food)
    stmt = stmt.limit(integer)
    #TODO: add conditional statement in case that selection is not available
    results = connection.execute(stmt).fetchall()
    for result in results:
        #TODO: Improve printing output, space bewtween entries.
        print('Name:{}\nAmenity:{}\nAddress:{}\nPhone:{}'.format(result[0],
              result[1], result[2] ,result[3]))
```

A possible query:

what_do_you_want_to_eat('regional',2)

Et voila:

Name:Nennella

Amenity:restaurant

Address:Vico Teatro Nuovo

Phone:NaN


Name:Eccellenze Campane

Amenity:restaurant

Address:Via Benedetto Brin

Phone:NaN

##Ideas about the Dataset
Two main observations on the provided data:

1-	The quantity of missing data in the element's child. It hinders the proper creation of what it might be a highly advantageous tool for an interested user. As a matter of example, 0ut of the 121 observations, just 25 of them count with a phone number, some of them, not valid ones. See the statistics regarding NaN Values in the 13 columns. In some of the cases we have as far as 117 missing entries out of 121.

         
count   13.0

mean    71.0

std     45.0

min      0.0

25%     52.0

50%     78.0

75%    111.0

max    117.0

2-	The quality of the data. The in the *cuisine* attribute seems rather loose. Apparently, it does not follow a standard, but most likely left upon the judgement of the entry's writer. The result is inaccurate and in some cases duplicated.

##Conclusion

I realized that the probability of creating ad-hoc data products based on the OSM files are enormously. Products that might be of great service to the communities involved.  It goes beyond tags like latitude, and longitude. Data collection and process must be upscaled to provide reliable information, but the experience has been not just rewarding, but completely enticing.
